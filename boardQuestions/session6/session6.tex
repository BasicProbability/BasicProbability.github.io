\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{amsmath, amssymb, enumerate, bbm, hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, urlcolor=blue}


\newcommand{\id}[1]{\mathbbm{1}\left(#1\right)}

\title{Board Questions Session 6}
\date{October 10th}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\begin{frame}{Geometric EM}
\small
You are given a mixture model with mixture components $ c_{1},c_{2} $ which are linked to geometric
distributions with parameters $ \theta^{(0)}_{c_{1}} = 0.2, \theta^{(0)}_{c_{2}} = 0.6 $. You
observe the data set
\begin{align*}
\{0, 2, 2, 3\} \ .
\end{align*}
Assume that the latent variables are i.i.d. and that $ P(Y=c_{1}|\Theta=\theta^{(0)}) = 0.2 $.
\begin{enumerate}[a)]
\item What is the (marginal) log-likelihood of this data set under the model? Feel free to use
calculators.
\item Find the most likely mixture component for each data point.
\item Perform one EM iteration.
\item Compute the marginal log-likelihood of the data with the updated parameters. The new value
should be higher than the one computed in the beginning.
\end{enumerate}
\end{frame}

\begin{frame}{Geometric EM}
\small
\begin{enumerate}[a)] 
\item We observed data $x_1, x_2, x_3, x_4 = 0, 2, 2, 3$; and have
  initial parameters
  $\theta^{(0)} = (\theta_{c_1}^{(0)}, \theta_{c_2}^{(0)},
  w_1^{(0)},w_2^{(0)}) = (0.2, 0.6, 0.2, 0.8)$. Therefore,
  the log-likelihood is: 
 \begin{align*}
  \log &P(X=x | \Theta=\theta^{(0)})\\
  = \log & P(X_1 = x_1 | \Theta=\theta^{(0)}) +  \log P(X_2 = x_2 |
        \Theta=\theta^{(0)})\\ 
        &+ \log P(X_3 = x_3 |
        \Theta=\theta^{(0)}) + \log P(X_4 = x_4 |
        \Theta=\theta^{(0)}) \\
   =\log &\big[P(X_1 = 0,Y_1=c_1 | \Theta=\theta^{(0)}) + P(X_1 =
          0,Y_1=c_2 | \Theta=\theta^{(0)}) \big]\\  
    &+\log P(X_2 = 2 |
        \Theta=\theta^{(0)}) + \log P(X_3 = 2 |
        \Theta=\theta^{(0)})\\ 
        &+ \log P(X_4 = 3 |
        \Theta=\theta^{(0)}) \\        
=\log&(0.2\cdot 0.2\cdot 0.8^{0} + 0.8 \cdot 0.6 \cdot 0.4^{0}) \\
&+ 2\cdot \log(0.2\cdot 0.2 \cdot 0.8^{2} + 0.8\cdot 0.6 \cdot 0.4^{2}) \\
&+ \log(0.2\cdot 0.2 \cdot 0.8^{3} + 0.8\cdot 0.6 \cdot 0.4^{3})
= \ensuremath{-8.1836793}
\end{align*}
\end{enumerate}
\end{frame}

\begin{frame}{Geometric EM}
\small
\begin{enumerate}[b)]
\item $ P(Y = c_{1}|X=0, \Theta=\theta^{(0)}) =
  \dfrac{0.2\cdot 0.2\cdot
    0.8^{0}}{0.2\cdot
    0.2\cdot 0.8^{0} + 0.8 \cdot 0.6 \cdot 0.4^{0}} = 0.0769231 $.\\ Also 
  $ P(Y = c_{2}|X=0, \Theta=\theta^{(0)})  = 1 -
  0.0769231 = 0.9230769$. Therefore, $c_2$
  is the more likely mixture component when observing $X=0$.
\\ \vspace{.4cm}
$ P(Y = c_{1}|X=2, \Theta=\theta^{(0)}) = \dfrac{0.2\cdot 0.2\cdot 0.8^{2}}{0.2\cdot 0.2\cdot 0.8^{2} + 0.8 \cdot 0.6 \cdot 0.4^{2}} = 0.25 $\\ Also 
  $ P(Y = c_{2}|X=2, \Theta=\theta^{(0)})  = 1 -
  0.25 = 0.75$. Therefore, $c_2$
  is the more likely mixture component when observing $X=2$.
\\ \vspace{.4cm}
$ P(Y = c_{1}|X=3, \Theta=\theta^{(0)}) = \dfrac{0.2\cdot 0.2\cdot 0.8^{3}}{0.2\cdot 0.2\cdot 0.8^{3} + 0.8 \cdot 0.6 \cdot 0.4^{3}} = 0.4 $\\ Also 
  $ P(Y = c_{2}|X=3, \Theta=\theta^{(0)})  = 1 -
  0.4 = 0.6$. Therefore, $c_2$
  is the more likely mixture component when observing $X=3$.
\end{enumerate}
  Go play with the \href{https://ocw.mit.edu/ans7870/18/18.05/s14/applets/probDistrib.html}{applet} in order to check that this makes sense!
\end{frame}

\begin{frame}{Geometric EM}
\small
\begin{enumerate}[c)]
\item \textbf{E-step}\begin{align*}
&\mbox{expected fractional count of $c_1$:}\\ 
                       &\mathbb{E}[\sum_{i}\id{Y_{i}=c_{1}}\mid X=x, \Theta=\theta^{(0)}] = 0.0769231 + 
2\cdot 0.25 + 0.4 \\
&= 0.9769231 \\[2mm]
&\mbox{expected fractional count of $c_2$:}\\ 
                       &\mathbb{E}[\sum_{i}\id{Y_{i}=c_{2}}\mid X=x, \Theta=\theta^{(0)}] = 4 - 0.9769231 = 3.0230769 \\[2mm]
&\mbox{expected sufficient statistic $\sum_i x_i$:}\\
&\mathbb{E}[\sum_{i}x_{i}\id{Y_{i}=c_{1}}\mid X=x,
  \Theta=\theta^{(0)}]\\  
                       &=0 \cdot 0.0769231 + 
2 \cdot 0.25 + 2 \cdot
  0.25 + 3 \cdot 0.4 = 2.2 \\
&\mathbb{E}[\sum_{i}x_{i}\id{Y_{i}=c_{2}}\mid X=x, \Theta=\theta^{(0)}]\\ 
                       &= 0 \cdot 0.9230769 + 
 2 \cdot 0.75 + 2 \cdot
                         0.75 +
                         3 \cdot 0.6 
                         = 4.8
\end{align*}
\end{enumerate}
\end{frame}



\begin{frame}{Geometric EM}
\textbf{M-step} For the MLE of the categorical RV $Y$, we normalize
the expected fractional counts computed in the E-step:
\begin{align*}
&P(Y=c_{1}\mid \Theta = \theta^{(1)}) = \frac{0.9769231}{0.9769231 + 3.0230769} = 0.2442308 \\
&P(Y=c_{2}\mid \Theta = \theta^{(1)}) = 1 - 0.2442308
= 0.7557692 
\end{align*}
We use that the MLE of this version of the geometric distribution is
given by $\theta_{MLE} = \frac{n}{n+\sum_i x_i}$, but now use the
fractional counts and expected sufficient statistic computed in the E-step:\\

new parameter of $ c_{1} $ : $ \dfrac{0.9769231}{
  0.9769231 + 2.2 } = 0.3075061 $\vspace{.5cm}

new parameter of $ c_{2} $ : $ \dfrac{3.0230769}{3.0230769 + 4.8} = 0.3864307 $
\end{frame}

\begin{frame}{Geometric EM}
 \begin{enumerate}[d)]
  The new log-likelihood of the data $x_1, x_2, x_3, x_4 = 0, 2, 2, 3$ is computed as in a) but now using the
  updated parameters $\theta^{(1)} = (\theta_{c_1}^{(1)}, \theta_{c_2}^{(1)},
  w_1^{(1)},w_2^{(1)}) = (0.3075061,
  0.3864307,0.2442308,0.7557692)$:
\item \begin{align*}
\log&(0.2442308\cdot 0.3075061\cdot 0.6924939^{0}  \\
&+ 0.7557692 \cdot 0.3864307 \cdot 0.6135693^{0}) \\
+ 2\cdot &\log(0.2442308\cdot 0.3075061 \cdot 0.6924939^{2} \\
&+ 0.7557692\cdot 0.3864307 \cdot 0.6135693^{2}) \\
+ \log&(0.2442308\cdot 0.3075061 \cdot 0.6924939^{3} \\
&+ 0.7557692\cdot 0.3864307 \cdot 0.6135693^{3})
= \ensuremath{-7.2323861} 
\end{align*}
\end{enumerate}
\end{frame}

\end{document}
